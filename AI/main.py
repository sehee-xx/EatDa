import os
import io
import asyncio
import time
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
from lumaai import AsyncLumaAI
from gms_api.gpt import generate_luma_prompt
import requests

load_dotenv()  # .env íŒŒì¼ ë¡œë“œ

app = FastAPI(
    title="AI Video Generation API",
    description="Luma AIë¥¼ ì‚¬ìš©í•œ ì˜ìƒ ìƒì„± API ì„œë²„",
    version="1.0.0"
)

# CORS ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ê°œë°œ í™˜ê²½ìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Luma AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    client = AsyncLumaAI(auth_token=os.getenv("LUMAAI_API_KEY"))
    print("âœ… Luma AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ Luma AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    client = None

generations = {}  # ë©”ëª¨ë¦¬ì— ì˜ìƒ ìƒì„± ì •ë³´ ì €ì¥

class GenerateRequest(BaseModel):
    prompt: str

class GenerateResponse(BaseModel):
    status: str
    message: str
    generation_id: str = None
    enhanced_prompt: str = None

@app.get("/")
async def root():
    """API ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {"message": "AI Video Generation API Server is running! ğŸš€"}

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy"}

@app.post("/generate", response_model=GenerateResponse)
async def generate_video(request: GenerateRequest):
    """
    ì˜ìƒ ìƒì„± ìš”ì²­ - luma.pyì˜ ë¡œì§ì„ FastAPIë¡œ êµ¬í˜„
    1. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ gpt.pyë¡œ ê°œì„ 
    2. ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¡œ Luma AI ì˜ìƒ ìƒì„± ìš”ì²­
    """
    try:
        if client is None:
            raise HTTPException(
                status_code=500, 
                detail="Luma AI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            )
        
        print(f"ğŸ“ ì›ë³¸ í”„ë¡¬í”„íŠ¸: {request.prompt}")
        
        # 1ë‹¨ê³„: GPTë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„  (gpt.py ì‚¬ìš©)
        print("âœ¨ GPTë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ëŠ” ì¤‘...")
        detailed_prompt = await generate_luma_prompt(request.prompt)
        print(f"ğŸ¯ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸: {detailed_prompt}")
        
        # 2ë‹¨ê³„: Luma AIë¡œ ì˜ìƒ ìƒì„± ìš”ì²­ (luma.py ë¡œì§ ì‚¬ìš©)
        print("ğŸ¬ Luma AIë¡œ ì˜ìƒ ìƒì„± ìš”ì²­ ì¤‘...")
        generation = await client.generations.create(
            prompt=detailed_prompt,
            model="ray-2",
            loop=True,
            aspect_ratio="9:16",
            duration="5s",
            keyframes={
                "frame0": {
                    "type": "image", 
                    "url": "https://storage.googleapis.com/be_my_logo/am_i_being_a_king.jpg"
                }
            }
        )
        
        # ë©”ëª¨ë¦¬ì— ìƒì„± ì •ë³´ ì €ì¥
        generations[generation.id] = {
            "state": generation.state,
            "original_prompt": request.prompt,
            "enhanced_prompt": detailed_prompt,
            "created_at": time.time()
        }
        
        print(f"âœ… ì˜ìƒ ìƒì„± ìš”ì²­ ì™„ë£Œ! ID: {generation.id}")
        
        return GenerateResponse(
            status="success",
            message="ì˜ìƒ ìƒì„± ìš”ì²­ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
            generation_id=generation.id,
            enhanced_prompt=detailed_prompt
        )
        
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì˜ìƒ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.get("/status/{generation_id}")
async def get_status(generation_id: str):
    """ì˜ìƒ ìƒì„± ìƒíƒœ í™•ì¸ - luma.pyì˜ polling ë¡œì§"""
    try:
        if generation_id not in generations:
            raise HTTPException(status_code=404, detail="ìƒì„± IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        if client is None:
            raise HTTPException(status_code=500, detail="Luma AI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # Luma AIì—ì„œ ìµœì‹  ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
        generation = await client.generations.get(id=generation_id)
        
        # ë©”ëª¨ë¦¬ì— ì €ì¥ëœ ì •ë³´ ì—…ë°ì´íŠ¸
        generations[generation_id]["state"] = generation.state
        
        response_data = {
            "generation_id": generation_id,
            "state": generation.state,
            "original_prompt": generations[generation_id].get("original_prompt"),
            "enhanced_prompt": generations[generation_id].get("enhanced_prompt")
        }
        
        # ìƒíƒœì— ë”°ë¥¸ ì¶”ê°€ ì •ë³´
        if generation.state == "completed":
            response_data["video_url"] = generation.assets.video
            response_data["message"] = "ğŸ‰ ì˜ìƒ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"
            print(f"âœ… ì˜ìƒ ìƒì„± ì™„ë£Œ: {generation_id}")
        elif generation.state == "failed":
            response_data["failure_reason"] = generation.failure_reason
            response_data["message"] = "âŒ ì˜ìƒ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤"
            print(f"âŒ ì˜ìƒ ìƒì„± ì‹¤íŒ¨: {generation_id}")
        else:
            response_data["message"] = "â³ ì˜ìƒ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... (2-5ë¶„ ì†Œìš”)"
            print(f"â³ ì˜ìƒ ìƒì„± ì¤‘: {generation_id} - {generation.state}")
        
        return response_data
        
    except Exception as e:
        print(f"âŒ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.get("/video/{generation_id}")
async def download_video(generation_id: str):
    """ì™„ì„±ëœ ì˜ìƒ ë‹¤ìš´ë¡œë“œ - luma.pyì˜ ë‹¤ìš´ë¡œë“œ ë¡œì§"""
    try:
        if generation_id not in generations:
            raise HTTPException(status_code=404, detail="ìƒì„± IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        if client is None:
            raise HTTPException(status_code=500, detail="Luma AI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        generation = await client.generations.get(id=generation_id)
        
        if generation.state != "completed":
            raise HTTPException(
                status_code=400, 
                detail=f"ì˜ìƒì´ ì•„ì§ ì™„ì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜„ì¬ ìƒíƒœ: {generation.state}"
            )
        
        video_url = generation.assets.video
        print(f"ğŸ“¥ ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {video_url}")
        
        # ì˜ìƒ íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë° ë‹¤ìš´ë¡œë“œ
        response = requests.get(video_url, stream=True)
        response.raise_for_status()
        
        return StreamingResponse(
            io.BytesIO(response.content), 
            media_type="video/mp4",
            headers={"Content-Disposition": f"attachment; filename={generation_id}.mp4"}
        )
        
    except Exception as e:
        print(f"âŒ ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.get("/generations")
async def list_generations():
    """ìƒì„±ëœ ì˜ìƒ ëª©ë¡ ì¡°íšŒ"""
    return {
        "total": len(generations),
        "generations": [
            {
                "id": gen_id,
                "state": info["state"],
                "original_prompt": info.get("original_prompt"),
                "created_at": info.get("created_at")
            }
            for gen_id, info in generations.items()
        ]
    }

# ì„œë²„ ì‹¤í–‰ (ê°œë°œìš©)
if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ AI Video Generation API ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
